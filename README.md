# MINISTRY-OF-WATER-RESOURCE
<b><i><U>WHAT IS SEARCH ENGINE OPTIMISATION?</U></i></b>

Search engine optimization (SEO) is the process of affecting the visibility of a website or a web page in a web search engine's unpaid results—often referred to as "natural", "organic", or "earned" results. In general, the earlier (or higher ranked on the search results page), and more frequently a site appears in the search results list, the more visitors it will receive from the search engine's users; these visitors can then be converted into customers.[1] SEO may target different kinds of search, including image search, local search, video search, academic search,[2] news search, and industry-specific vertical search engines.

As an Internet marketing strategy, SEO considers how search engines work, what people search for, the actual search terms or keywords typed into search engines and which search engines are preferred by their targeted audience. Optimizing a website may involve editing its content, HTML, and associated coding to both increase its relevance to specific keywords and to remove barriers to the indexing activities of search engines. Promoting a site to increase the number of backlinks, or inbound links, is another SEO tactic.<br>

HERE IS AN EXAMPLE OF SEO DONE ON A GOVERNMENT WEBSITE-<b> MINISTRY OF COAL </b>:-
<html>
<body>
<table>
<font size=5>
<table border=5 > 
<tr><th> FAULT <th>DESCRIPTION<th>USAGE/IMPORTANCE<th><th>SUGGESTION<th></tr>


<tr>


<th> meta description</th>

<td>The meta description is a ~160 character snippet, a tag in HTML, that 

summarizes a page's content. Search engines show the meta description in search 

results mostly when the searched for phrase is contained in the description. 

Optimizing the meta description is a very important aspect of on-page SEO. <td>
<td>The meta description for this website should be only upto 160 

characters.</td>
<td>Meta description tags, while not important to search engine rankings, are extremely important in gaining user click-through from SERPs. These short paragraphs are a webmaster's opportunity to advertise content to searchers and to let them know exactly whether the given page contains the information they're looking for.
<br>
</tr>

<tr>
<th> Headings Status	</th>
<td>Heading tags are imperative for every web 

document to structure the content. By using Heading Tags, we differentiate our web 

page content.The six heading elements, H1 through H6, denote section headings. 

</td>
<td> No Heading is found. <b><u>Suggested</u></b>---Ministry of coal</td>
<td> While keyword prominence is still an important ranking factor, whether it is within the H1 or simply highlighted at the top of the page, there is another level to the effect of the H1 tag on organic search performance. A page's ability to effectively engage users is another way Google organizes its search results.</td>
<td>The H1 heading should contain the main subject of a page. Use only one H1 on a web page.</td>



<br>
</tr>

<tr>
<th>Robots.txt Test	</th>
<td>The robots exclusion standard, also known 

as the robots exclusion protocol or simply robots.txt, is a standard used by websites 

to communicate with web crawlers and other web robots. The standard specifies 

how to inform the web robot about which areas of the website should not be 

processed or scanned.</td>
<td>Not found on the website. Robots.txt is needed as it 

specifies how to inform the web robot about which areas of the website should not 

be processed or scanned. </td>
<td>Your Robots.txt file is what tells the search engines which pages to access and index on your website on which pages not to.

For example, if you specify in your Robots.txt file that you don’t want the search engines to be able to access your thank you page, that page won’t be able to show up in the search results and web users won’t be able to find it.

Keeping the search engines from accessing certain pages on your site is essential for both the privacy of your site and for your SEO.</td>
<br>
</tr>

<tr>
<th>SEO Friendly URL Test</th>
<td> In order for links to be SEO friendly, they 

should contain keywords relevant to the page's topic, and contain no spaces, 

underscores or other characters. You should avoid the use of parameters when 

possible, as they make URLs less inviting for users to click or share.</td>
<td> The 

URL of the website is not SEO friendly.The URL should contain keywords relevant to 

the page's topic, and contain no spaces, underscores or other characters.</td>
<td>Clearly the friendly URL has the advantage. URLs are not a large ranking factor in and of themselves, but there are two additional effects to consider:

1. Friendly URLs affect CTR. Many users will examine a URL when deciding whether to click through or not.

2. Friendly URLs improve anchor text link juice flow. If your page URL is /watches then any links where the referring site simply copies your URL will have your product name (watches) in the URL.</td>
<td>The robots exclusion protocol (REP), or robots.txt is a text file webmasters create to instruct robots (typically search engine robots) how to crawl and index pages on their website.

Robots.txt needs to be placed in the top-level directory of a web server in order to be useful. Example: http://www.example.com/robots.txt</td>

<br>
</tr>


<tr>
<th>Domain Authority	</th>
<td>Domain authority is a measure of the power 

of a domain name and is one of many search engine ranking factors. Domain 

authority is based on three factors: Age, Popularity, and Size. SEO gurus Moz can be 

credited with the metric known as DA or domain authority.Domain Authority (DA) is 

a score developed by Moz that predicts how well a website will rank on search 

engine result pages (SERP). Domain Authority scores range from one to 100, with 

higher scores corresponding to a greater ability to rank.
Sites with a very large 

number of high-quality external links (such as Wikipedia or Google.com) are at the 

top end of the Domain Authority scale.</td>
<td> This site should put a large 

number of high-quality links to gain a good domain authority and be at the top end 

of the DA scale</td>
<td>Domain authority is a measure of the power of a domain name and is one of many search engine ranking factors.
Domain Authority metrics are incorporated into dozens of SEO and online marketing platforms across the web.

SEO gurus Moz can be credited with the metric known as DA or domain authority. 

Moz’s own search algorithm gauges the quality of any given site based on a huge complex combination of factors including onsite and offsite, taking into account things such as diversity of Backlinking domains to come up with a score between 0 and 100. A brand new site, for example, will have 0 whereas a very high authority site might have 80/100.

It is a measure of the power of a domain name and is one of many search engine ranking factors. Domain authority is based on three factors: Age, Popularity, and Size. </td>
<td>3 Best Techniques to Increase Domain Authority (DA) of a Website:
<u>Deep Linking:</u>

When you are building an authoritative website, make sure that all the inner pages of your blog/website are strongly built to each other. Create authority for all the pages you have in your website, and optimize each page for the specific keyword it is targeting. You should tell Google that all the internal pages are as much as important to rank well, just like the home page or any other popular page of your website. So we need to build a lot of backlinks for internal pages in order to show up in Google.
<u>Link Diversity:</u>

Link Diversity is an Off-Page SEO concept you need to understand for better Domain Authority rank. Link diversity is a SEO term which is used to decide the range of different links pointing to your website. If you are trying to get similar kind of links from a site again and again, Google doesn’t give much value to that page or URL you are trying to rank on. Instead, its better to diversify the links from different sites and sources, so that each link is calculated as a different entity to your targeted page.

<u> Quality Content:</u>

You know the fact that a website should contain high quality content to rank up in Google. So how is this related to Domain Authority? Well, it is! After the Google panda update, many websites lost their authority in Internet, as they lost their low quality pages in search engine positions. When you write high quality content, people do link it from their websites, other resources to share the content among their networks. So the links which point to your website from other websites is an added advantage to your site popularity. There are many other metrics you need to follow to keep your content quality rich.</td>

</tr>

<tr>
<th>Page Authority</th>
<td>Page Authority is a score (on a 100-point scale) 

developed by Moz that predicts how well a specific page will rank on search 

engines. It is based off data from the Mozscape web index and includes link counts, 

MozRank, MozTrust, and dozens of other factors.it's best used as a comparative 

metric (rather than an absolute, concrete score) when doing research in the search 

results and determining which pages may have more powerful or important link 

profiles than others. Because it's a comparative tool, there isn't necessarily a "good" 

or "bad" Page Authority score.	
</td>
<td>The website should put powerful 

and important link profiles than others and will thus, gain a good page authority. 

</td>
<td>PA defines the links that you are getting on that particular page and this varies from one URL to another of the same domain.
How is Page Authority scored?
We score Page Authority on a 100-point logarithmic scale. Thus, it's significantly easier to grow your score from 20 to 30 than it is to grow from 70 to 80. We constantly update the algorithm used to calculate Page Authority, so you may see your score fluctuate from time to time.
t's best used as a comparative metric (rather than an absolute, concrete score) when doing research in the search results and determining which pages may have more powerful or important link profiles than others. Because it's a comparative tool, there isn't necessarily a "good" or "bad" Page Authority score.
 For increase in PA you have to increase the links on that particular page.
 <td>Strategies to Improve The Page Authority
 
 <u>High Quality Backlinks:</u>

Getting back-links to your website is an amazing art, which majority of us fail initially. We know the best way of getting a backlink is through guest blogging. But how about get a high quality link without even guest blogging about it? That’s where the concept of high quality articles come into the picture. Just as discussed in High Quality Content, all you have to do is to write good content and wait for it to generate links automatically. If not you have to apply the standard approach of getting quality links through Guest Blogging.

<u>Good Internal Backlink Structure:</u>

Your website should always have a deep inter linking structure, which you have to closely monitor every week. Identify the high authority pages within your website, and make sure to use them on fresh/newly built pages. This will boost your website traffic as internal linking works amazingly most of the times. Make sure your website doesn’t contain more than 3 layer linking system, as too many internal links might not help the website to rank better in Google.</td><br>
</tr>



<tr>
<th> WWW redirection Test</th>
<td>Many directories make redirects to external 

sites. As search engines are still mislead by some types of redirects, webmasters 

need to know which type of redirection is used to their site.</td>
<td>The website 

should have a redirect checker to regulate redirections</td>
<td>For what seems like forever, SEOs have operated by a set of best practices that dictate how to best handle redirection of URLs. (This is the practice of pointing one URL to another. If you need a quick refresher, here’s a handy guide on HTTP status codes.)

These tried and true old-school rules included:

301 redirects result in around a 15% loss of PageRank. Matt Cutts confirmed this in 2013 when he explained that a 301 loses the exact same amount of PageRank as a link from one page to another.
302s don’t pass PageRank. By definition, 302s are temporary. So it makes sense for search engines to treat them different.
HTTPS migrations lose PageRank. This is because they typically involve lots of 301 redirects.
These represent big concerns for anyone who wants to change a URL, deal with an expired product page, or move an entire website.
</td>
<td>It is common practice to redirect one URL to another. When doing this, it is critical to observe best practices in order to maintain SEO value.

The first common example of this takes place with a simple scenario: a URL that needs to redirect to another address permanently.



There are multiple options for doing this, but in general, the 301 redirect is preferable for both users and search engines. Serving a 301 indicates to both browsers and search engine bots that the page has moved permanently. Search engines interpret this to mean that not only has the page changed location, but that the content—or an updated version of it—can be found at the new URL. The engines will carry any link weighting from the original page to the new URL, as below:



Be aware that when moving a page from one URL to another, the search engines will take some time to discover the 301, recognize it, and credit the new page with the rankings and trust of its predecessor. This process can be lengthier if search engine spiders rarely visit the given web page, or if the new URL doesn't properly resolve.

Other options for redirection, like 302s and meta refreshes, are poor substitutes, as they generally will not pass the rankings and search engine value like a 301 redirect will. The only time these redirects are good alternatives is if a webmaster purposefully doesn't want to pass link juice from the old page to the new.

Transferring content becomes more complex when an entire site changes its domain or when content moves from one domain to another. Due to abuse by spammers and suspicion by the search engines, 301s between domains sometimes require more time to be properly spidered and counted. For more on moving sites, see Achieving an SEO-Friendly Domain Migration: The Infographic.</td>


<tr>
<th>HTML Page Size Test</th>
<td>HTML size is the size of all the HTML code on 

your web page - this size does not include images, external javascripts or external 

CSS files.</td>
<td>Your HTML size is 76.17 Kb and this is above the average web 

page size of 33 Kb.This leads to a slower page loading time than average.Lowerind 

the size of the page is reckoned.</td>
<td>Web page size is an important factor in determining how well optimized your site is. Search engines like MSN, Yahoo and Google are considering load time to be more and more important when it comes to both search engine optimization and PPC quality scores, and file size certainly effects your website’s load time. However, many web developers have a hard time determining which page sizes and file sizes are ideal for making sure that their website loads quickly. In this article, we’ll be looking at some data which has been gathered about the average size of HTML web pages, image files and other components of a website. This will help web developers to determine a solid guideline for their page sizes and file sizes.
</td>
<td>Web page speed report provides exhaustive information on a page size including:

Web Page Speed Report

total page size;
total size of the images (and HTML and CSS images separately);
JavaScript size;
CSS size;
Each page object size;
And download times for a set of connection rates:
Web Page Speed Report - download times

What’s more, the tool will also provide you with the list of useful recommendations on reducing size of particular elements:

Web Page Speed Report - Recommendations

Page Size Extractor offers a short but very handy analysis :

Page Size Extractor

Total page size;
Text to HTML ratio;
Total hyperlinks number;
Total images number;
Total size of all images;
Each image size;
The full list of all links on the page.
Web Developer FireFox Extension is another handy utility that can show each image size right within a page.</td>


<br>
</tr><br>
</tr>


<tr>
<th>HTML Compression/GZIP Test</th>
<td>Gzip is a method of compressing 

files (making them smaller) for faster network transfers. It is also a file format. 

Compression allows your web server to provide smaller file sizes which load faster 

for your website users. Enabling gzip compression is a standard practice.</td>
<td> 

It is recommended to use html compression e-g: gzip compression</td>
<td>What is Gzip compression?
Gzip is a method of compressing files (making them smaller) for faster network transfers.
It is also a file format.Compression allows your web server to provide smaller file sizes which load faster for your website users. </td>

<td>Compression is a simple, effective way to save bandwidth and speed up your site. 
<b><i>How To Optimize Your Site With GZIP Compression:-</i></b>
https://betterexplained.com/articles/how-to-optimize-your-site-with-gzip-compression/</td>
<td>Your two options for file compression are Deflate and GZIP. Deflate is an option which comes automatically with the Apache server and which is simple to set up. GZIP on the other hand needs to be installed and requires a bit more work to install. However, GZIP does achieve a higher compression rate and therefore, might be a better choice if your website uses pages which have a lot of images or large file sizes.

On the other hand, if your site has over 1,000 unique visitors a day, deflate is actually a better selection because it requires much less energy to compress files, which means your high volume of visitors won’t slow down your site due to the increase in GZIP compression.  

Setting up File Compression with Deflate

Setting up file compression for your website will depend on which type of server you’re using for your website. Most likely, you’ll be using Apache, which means you can enable compression by adding a few deflate codes to your .htaccess file. This file can be found by accessing your websites control and command (CNC) panel and is located in your main directory. For example, if your site was http://www.example.com, your .htaccess file would be in this location:

http://www.example.com/.htaccess

Just keep in mind that you won’t be able to view your .htaccess file simply by typing this URL into your browser. You have to access the file location through your CNC panel. Then, you simply add the codes to your .htaccess file which will compress your site’s files by types. For example, if you want to compress all of the .txt and HTML files on your site:

AddOutputFilterByType DEFLATE text/plain

AddOutputFilterByType DEFLATE text/html

-Other codes can be added to the .htaccess file to compress your .xml files:

AddOutputFilterByType DEFLATE text/xml

AddOutputFilterByType DEFLATE application/xml</td><br>
</tr>

</table>
</body>
</html>>








